{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "\n",
    "Analytics Industry is all about obtaining the “Information” from the data. With the growing amount of data in recent years, that too mostly unstructured, it’s difficult to obtain the relevant and desired information. But, technology has developed some powerful methods which can be used to mine through the data and fetch the information that we are looking for.\n",
    "\n",
    "One such technique in the field of text mining is Topic Modelling. As the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n",
    "\n",
    "Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.\n",
    "\n",
    "Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Topic Modeling? Why do we need it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large amounts of data are collected everyday. As more information becomes available, it becomes difficult to access what we are looking for. So, we need tools and techniques to organize, search and understand vast quantities of information.\n",
    "\n",
    "Topic modelling provides us with methods to organize, understand and summarize large collections of textual information. It helps in:\n",
    "\n",
    "Discovering hidden topical patterns that are present across the collection\n",
    "Annotating documents according to these topics\n",
    "Using these annotations to organize, search and summarize texts\n",
    "Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought of as a form of text mining – a way to obtain recurring patterns of words in textual material.\n",
    "\n",
    "There are many techniques that are used to obtain topic models. This post aims to explain the Latent Dirichlet Allocation (LDA): a widely used topic modelling technique and the TextRank process: a graph-based algorithm to extract relevant key phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LDA model, each document is viewed as a mixture of topics that are present in the corpus. The model proposes that each word in the document is attributable to one of the document’s topics.\n",
    "\n",
    "For example, consider the following set of documents as the corpus:\n",
    "\n",
    " \n",
    "\n",
    "Document 1: I had a peanut butter sandwich for breakfast.\n",
    "Document 2: I like to eat almonds, peanuts and walnuts.\n",
    "Document 3: My neighbor got a little dog yesterday.\n",
    "Document 4: Cats and dogs are mortal enemies.\n",
    "Document 5: You mustn’t feed peanuts to your dog.\n",
    "\n",
    "The LDA model discovers the different topics that the documents represent and how much of each topic is present in a document. For example, LDA may produce the following results:\n",
    "\n",
    "Topic 1: 30% peanuts, 15% almonds, 10% breakfast… (you can interpret that this topic deals with food)\n",
    "Topic 2: 20% dogs, 10% cats, 5% peanuts… ( you can interpret that this topic deals with pets or animals)\n",
    "\n",
    "Documents 1 and 2: 100% Topic 1\n",
    "Documents 3 and 4: 100% Topic 2\n",
    "Document 5: 70% Topic 1, 30% Topic 2\n",
    "\n",
    "So, how does LDA perform this process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"C:/Users/souham/Desktop/NLP/lda.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collapsed Gibbs sampling is one way the LDA learns the topics and the topic representations of each document. The procedure is as follows:\n",
    "\n",
    "Go through each document and randomly assign each word in the document to one of K topics (K is chosen beforehand)\n",
    "This random assignment gives topic representations of all documents and word distributions of all the topics, albeit not very good ones\n",
    "So, to improve upon them:\n",
    "For each document d, go through each word w and compute:\n",
    "p(topic t | document d): proportion of words in document d that are assigned to topic t\n",
    "p(word w| topic t): proportion of assignments to topic t, over all documents d, that come from word w\n",
    "Reassign word w a new topic t’, where we choose topic t’ with probability\n",
    "p(topic t’ | document d) * p(word w | topic t’)\n",
    "This generative model predicts the probability that topic t’ generated word w\n",
    "On repeating the last step a large number of times, we reach a steady state where topic assignments are pretty good. These assignments are then used to determine the topic mixtures of each document.\n",
    "\n",
    "TextRank[2]\n",
    "Graph-based ranking algorithms are a way of deciding the importance of a vertex within a graph, based on the information derived from the entire graph. The basic idea, implemented by a graph-based ranking model, is that of “voting”.\n",
    "\n",
    "When one vertex links to another one, it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex. Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model. Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.\n",
    "\n",
    "The score for each vertex, Vi is calculated as:\n",
    "\n",
    "\n",
    "Here, G = (V, E) is a directed graph with a set of vertices V and a set of edges E. For a given vertex, Vi, In(Vi) denotes the number of inward edges to that vertex and Out(Vi) denotes the number of outward edges from that vertex. d is the damping factor which is set to 0.85, as is done in PageRank[3]. Now to enable the application of this model to natural language texts, we follow the steps:\n",
    "\n",
    "Identify text units that best define the task at hand, and add them as vertices in the graph.\n",
    "Identify relations that connect such text units, and use these relations to draw edges between vertices in the graph. Edges can be directed or undirected, weighted or unweighted.\n",
    "Iterate the graph-based ranking algorithm until convergence.\n",
    "Sort vertices based on their final score. Use the values attached to each vertex for ranking / selection decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"C:/Users/souham/Desktop/NLP/tree.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allows us to obtain relevant keyphrases for each document in the collection. So, in order to obtain relevant topics from the entire collection, we apply the same procedure, where each vertex in the graph denotes relevant keyphrases of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.029*\"driving\" + 0.029*\"dance\" + 0.029*\"time\" + 0.029*\"father\" + 0.029*\"around\" + 0.029*\"a\" + 0.029*\"practice.\" + 0.029*\"spends\" + 0.029*\"of\" + 0.029*\"lot\"'), (1, '0.060*\"driving\" + 0.060*\"cause\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"that\" + 0.060*\"blood\" + 0.060*\"increased\" + 0.060*\"stress\" + 0.060*\"pressure.\" + 0.060*\"may\"'), (2, '0.083*\"to\" + 0.058*\"My\" + 0.058*\"sister\" + 0.058*\"my\" + 0.033*\"sugar,\" + 0.033*\"not\" + 0.033*\"Sugar\" + 0.033*\"is\" + 0.033*\"bad\" + 0.033*\"consume.\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim \n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
